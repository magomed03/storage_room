{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQlcDy9BC5bE"
      },
      "source": [
        "# Методы машинного обучения – Лабораторная работа №2\n",
        "\n",
        "# Нелинейная регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKlRxa2EKcTu"
      },
      "source": [
        "Линейной регрессии может быть недостаточно для выявления взаимосвязи между признаками $X_{1},X_{2},...,X_{d}$ и откликом $Y$ в случае, когда эта взаимосвязь является нелинейной, поэтому приходится рассматривать более общий случай, когда \n",
        "\n",
        "$$Y=f\\left(\\mathbf{X}\\right)+\\varepsilon=f\\left(X_{1},X_{2},...,X_{d}\\right)+\\varepsilon,$$\n",
        "\n",
        "где $\\varepsilon$ – случайная ошибка, которая предполагается независимой от многомерной случайной величины $\\mathbf{X}=\\left(X_{1},X_{2},...,X_{d}\\right)^{T}\\in\\mathbb{R}^{d}$, причем $\\mathbb{E}\\left[\\varepsilon\\right]=0$. \n",
        "\n",
        "Импортируем необходимые библиотеки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCNmPhYuurZN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea4FLN13u0ER"
      },
      "source": [
        "При рассмотрении нелинейной регресии часто применяют градиентные методы.\n",
        "\n",
        "### Моделирование градиентного спуска\n",
        "\n",
        "Градиентный спуск — метод нахождения локального минимума или максимума функции с помощью движения вдоль градиента. \n",
        "\n",
        "Изобразим график функции $y = (x-2.5)^2-1$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB8EvIk5vDFY"
      },
      "outputs": [],
      "source": [
        "plot_x = np.linspace(-1., 6., 141)\n",
        "plot_y = (plot_x-2.5)**2 - 1.\n",
        "plt.plot(plot_x, plot_y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd4ylPG8MVHD"
      },
      "source": [
        "Минимум этой функции достигается в точке $x=2.5$ и равен $-1$.\n",
        "\n",
        "Для вычисления значений и производной функции $y = (x-2.5)^2-1$ будем использовать следующие функции:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTwVnLapvLbP"
      },
      "outputs": [],
      "source": [
        "def J(x_):\n",
        "    return (x_-2.5)**2 - 1.\n",
        "\n",
        "def dJ(x_):\n",
        "    return 2*(x_-2.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfbCnAOmvo_n"
      },
      "source": [
        "Для моделирования градиентного спуска будем использовать функцию `gradient_descent()`, которая будет запоминать и возвращать историю итераций, для визуализации градиентного спуска будем использовать функцию `plot_history()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7APqhK-vz6X"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(initial_x, eta, n_iters = 1e4, epsilon=1e-8):\n",
        "    x_ = initial_x\n",
        "    x_history = [initial_x]\n",
        "    i_iter = 0\n",
        "\n",
        "    while i_iter < n_iters:\n",
        "        gradient = dJ(x_)\n",
        "        last_x_ = x_\n",
        "        x_ -= eta * gradient\n",
        "        x_history.append(x_)\n",
        "    \n",
        "        if(abs(J(x_) - J(last_x_)) < epsilon):\n",
        "            break\n",
        "        i_iter += 1\n",
        "        \n",
        "    return x_history\n",
        "            \n",
        "def plot_history(plot_x, x_history):\n",
        "    plt.plot(plot_x, J(plot_x))\n",
        "    plt.plot(np.array(x_history), J(np.array(x_history)), color=\"r\", marker='+')\n",
        "    plt.text(1., 10., f'Кол-во шагов: {len(x_history)}', fontsize=14, color='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09jU4noIwExK"
      },
      "source": [
        "Проведем моделирование градиентного спуска с различной скоростью:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjLFRgqhwY1H"
      },
      "outputs": [],
      "source": [
        "hist = gradient_descent(0., 0.1) # скорость 0.1\n",
        "plot_history(plot_x, hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZrLzfyhMVHG"
      },
      "outputs": [],
      "source": [
        "hist = gradient_descent(0., 0.01) # скорость 0.01\n",
        "plot_history(plot_x, hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diIu6wJ2MVHH"
      },
      "outputs": [],
      "source": [
        "hist = gradient_descent(0., 0.001) # скорость 0.001\n",
        "plot_history(plot_x, hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmOGB4w1MVHI"
      },
      "outputs": [],
      "source": [
        "hist = gradient_descent(0., 0.8) # скорость 0.8\n",
        "plot_history(plot_x, hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95uN08DfCWtf"
      },
      "source": [
        "При дальнейшем увеличении скорости возникает ошибка, которую нужно обработать:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxxozfeOC4gL"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    hist = gradient_descent(0., 1.1) # скорость 1.1\n",
        "    plot_history(plot_x, hist)\n",
        "except Exception as e: \n",
        "    print(f\"{type(e).__name__}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hwtgwQUDzNP"
      },
      "source": [
        "Поэтому ограничимся десятью итерациями:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qapfJX42MVHL"
      },
      "outputs": [],
      "source": [
        "hist = gradient_descent(0., 1.1, n_iters=10)\n",
        "plot_history(plot_x, hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTgbNgA1TSFd"
      },
      "source": [
        "Таким образом, при использовании градиентного спуска важную роль играет скорость (размер шага). \n",
        "\n",
        "### Стохастический градиентный спуск\n",
        "\n",
        "Стохастический градиентный спуск (stochastic gradient descent, SGD) − оптимизационный алгоритм, отличающийся от обычного градиентного спуска тем, что градиент оптимизируемой функции считается на каждом шаге не как сумма градиентов от каждого элемента выборки, а как градиент от одного, случайно выбранного элемента или некоторой подвыборки.\n",
        "\n",
        "Рассмотрим следующий синтетический набор данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL49HDihTSFe"
      },
      "outputs": [],
      "source": [
        "m = 100000 # количество точек в наборе\n",
        "\n",
        "x = np.random.normal(size=m)\n",
        "X = x.reshape(-1,1)           # преобразуем вектор в матрицу с одним столбцом\n",
        "y = 4.*x + 3. + np.random.normal(0, 3, size=m)\n",
        "\n",
        "plt.scatter(x, y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxvjPYbGTSFf"
      },
      "source": [
        "Определим класс `RegressionSGD`, использующий стохастический градиентный спуск (с переменным шагом) при обучении модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Wzl1eJMVHO"
      },
      "outputs": [],
      "source": [
        "class RegressionSGD:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self._theta = None\n",
        "\n",
        "    def fit(self, X_train, y_train, n_iters=50, t0=5, t1=50):\n",
        "        assert X_train.shape[0] == y_train.shape[0], \\\n",
        "            \"Размер X_train должен быть равен размеру y_train\"\n",
        "        assert n_iters >= 1\n",
        "\n",
        "        def dJ_sgd(theta, X_b_i, y_i):\n",
        "            return X_b_i * (X_b_i.dot(theta) - y_i) * 2.\n",
        "\n",
        "        def sgd(X_b, y, initial_theta, n_iters=5, t0=5, t1=50):\n",
        "\n",
        "            def learning_rate(t):\n",
        "                return t0 / (t + t1)\n",
        "\n",
        "            theta = initial_theta\n",
        "            m = len(X_b)\n",
        "            for i_iter in range(n_iters):\n",
        "                indexes = np.random.permutation(m)\n",
        "                X_b_new = X_b[indexes,:]\n",
        "                y_new = y[indexes]\n",
        "                for i in range(m):\n",
        "                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])\n",
        "                    theta = theta - learning_rate(i_iter * m + i) * gradient\n",
        "\n",
        "            return theta\n",
        "\n",
        "        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])\n",
        "        initial_theta = np.random.randn(X_b.shape[1])\n",
        "        self._theta = sgd(X_b, y_train, initial_theta, n_iters, t0, t1)\n",
        "\n",
        "        self.intercept_ = self._theta[0]\n",
        "        self.coef_ = self._theta[1:]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_predict):\n",
        "        assert self.intercept_ is not None and self.coef_ is not None, \\\n",
        "            \"Нужно обучить модель перед использованием!\"\n",
        "        assert X_predict.shape[1] == len(self.coef_), \\\n",
        "            \"Кол-во признаков в X_predict должно быть равно кол-ву признаков в X_train\"\n",
        "\n",
        "        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])\n",
        "        return X_b.dot(self._theta)\n",
        "\n",
        "    def score(self, X_test, y_test):\n",
        "        y_predict = self.predict(X_test)\n",
        "        return r2_score(y_test, y_predict)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"RegressionSGD()\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqc61L5wMVHP"
      },
      "source": [
        "Используем созданный класс для обучения модели линейной регрессии на сгенерированных ранее данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajVaffnRMVHQ"
      },
      "outputs": [],
      "source": [
        "reg = RegressionSGD()\n",
        "reg.fit(X, y, n_iters=2)\n",
        "reg.coef_, reg.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r59QI3SqMVHQ"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y)\n",
        "plot_x = np.linspace(-4, 4, 101)\n",
        "plt.plot(plot_x, reg.predict(plot_x.reshape(-1,1)), c='r', lw=10);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FWTQ7uOeVNS"
      },
      "source": [
        "### Полиномиальная регрессия\n",
        "\n",
        "Полиномиальная регрессия – это форма регрессионного анализа, в которой взаимосвязь между независимой переменной $X$ и зависимой переменной $Y$ моделируется как полином $m$-й степени от $X$\n",
        "\n",
        "$$Y=f\\left(X,\\mathbf{w}\\right)=w_{0}+w_{1}X+w_{2}X^{2}+...+w_{m}X^{m}+\\varepsilon$$\n",
        "\n",
        "Полиномиальная регрессия может использоваться для решения задачи регрессии для нелинейных данных. В полиномиальной регрессии используется кривая линия, соответствующая полиному степени больше, чем 1. Например, пусть входные данные соответствуют зависимости $y=0.5 x^2+x+2$ (с нормальным шумом):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU1CtmGcMVHR"
      },
      "outputs": [],
      "source": [
        "x = np.random.uniform(-3, 3, size=100) # вектор\n",
        "X = x.reshape(-1, 1)                   # матрица с одним стобцом \n",
        "y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)\n",
        "\n",
        "plt.scatter(x, y);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGCeZCQyev5U"
      },
      "outputs": [],
      "source": [
        "reg = RegressionSGD()\n",
        "reg.fit(X, y, n_iters=2)\n",
        "y_predict = reg.predict(X)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, y_predict, color='r');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAi-9ebdMVHS"
      },
      "source": [
        "Подготовим для модели регрессии входные данные с двумя признаками – линейной и квадратичной зависимостью от независимой переменной:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS1_oZaJMVHS"
      },
      "outputs": [],
      "source": [
        "X2 = np.hstack([X, X**2]) # соединение массивов по горизонтали\n",
        "X2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piKe6zUEMVHS"
      },
      "source": [
        "Применим к построенным данным модель регрессии на основе SGD и нарисуем набор данных и линию регрессии (функция `np.argsort` возвращает индексы элементов в отсортированном массиве):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB4SM5yie75R"
      },
      "outputs": [],
      "source": [
        "reg2 = RegressionSGD()\n",
        "reg2.fit(X2, y, n_iters=2000)\n",
        "y_predict2 = reg2.predict(X2)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(np.sort(x), y_predict2[np.argsort(x)], c='r', lw=3); "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если не использовать функции `sort()` и `argsort()`, то график выглядит так:"
      ],
      "metadata": {
        "id": "H--_EsUE_z0Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAq8tF0PMVHT"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y_predict2, c='r');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE7EFCw6MVHT"
      },
      "source": [
        "Определенные при помощи стохастического градиентного спуска коэффициенты регрессии и смещение близки к значениям, использованным при генерации данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy2nQjTCMVHT"
      },
      "outputs": [],
      "source": [
        "reg2.coef_, reg2.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Альтернативный способ построения наборов данных, содержащих полиномиальные зависимости от исходных данных, состоит в применении класса `PolynomialFeatures` из библиотеки `scikit-learn`:"
      ],
      "metadata": {
        "id": "Jf1R4m89Y5yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2) \n",
        "poly.fit(X)\n",
        "X2s = poly.transform(X)\n",
        "X2s"
      ],
      "metadata": {
        "id": "DS6dmRxBZT5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VszoyEHbMVHU"
      },
      "source": [
        "### Полиномиальная регрессия при помощи TensorFlow\n",
        "\n",
        "Полиномиальная регрессия также может быть реализована при помощи `TensorFlow` и `keras`. \n",
        "\n",
        "Математически __искусственная нейронная сеть__ (прямого распространения) представляет собой направленный граф с нейронами в качестве вершин и связями между нейронами в виде ребер, причем вход для каждого нейрона является функцией взвешенной суммы выходов всех нейронов, связанных с ним входящими ребрами. Тогда выход нейронной сети равен \n",
        "\n",
        "$$\\mathbf{f\\left(\\mathbf{x};\\theta\\right)=\\psi_{d}\\left(...\\psi_{2}\\left(\\psi_{1}\\left(\\mathbf{x}\\right)\\right)\\right),\\,\\psi_{i}\\left(\\mathbf{x}\\right)=\\sigma_{i}\\left(\\mathbf{w^{\\left(i\\right)}\\,x+b^{\\left(i\\right)}}\\right)}$$\n",
        "\n",
        "Здесь каждый слой сети представляется __функцией активации__ $\\mathbf{\\sigma_{i}}$ с аргументом в виде взвешенной суммы входных данных $\\mathbf{x}$ с __весами__ $\\mathbf{w^{\\left(i\\right)}}$ и __смещениями__ $\\mathbf{b^{\\left(i\\right)}}$. Число слоев $\\mathbf{d}$ называется __глубиной__ нейронной сети и количество нейронов в слое представляет собой __ширину__ этого слоя. \n",
        "\n",
        "Целью глубокого обучения является определение набора параметров сети $\\mathbf{\\theta=\\left\\{ w^{\\left(i\\right)},b^{\\left(i\\right)}\\right\\} _{i=1}^{d}}$, который минимизирует __функцию потерь__ $\\mathbf{\\mathcal{L}(\\theta)}$, определяющую качество модели при заданном наборе параметров $\\theta$. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим простейшую нейронную сеть с одним слоем из одного нейрона и двумя входными нейронами:"
      ],
      "metadata": {
        "id": "bn08cqBzugUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj1gY12zMVHU"
      },
      "outputs": [],
      "source": [
        "reg2_model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(2,)),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGEl2D3LMVHU"
      },
      "source": [
        "В такой нейронной сети будет всего три обучаемых параметра (два веса и смещение):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0vec6ryMVHU"
      },
      "outputs": [],
      "source": [
        "reg2_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EjKsywuMVHV"
      },
      "source": [
        "При компиляции модели в обязательном порядке указывается функция потерь (`loss`), также может быть выбран оптимизатор с параметрами (`optimizer`), метрики для оценки обучения (`metrics`) и некоторые другие другие параметры. В качестве функции потерь и/или метрики могут быть, в частности,  выбраны среднеквадратичная ошибка (MSE) и средняя абсолютная ошибка (MAE). Коэффициента детерминации $R^2$ среди стандартных функций потерь и метрик нет, но он легко может быть вычислен непосредственно по показателю MSE и общей дисперсии целевой переменной."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jelC5zS0MVHV"
      },
      "outputs": [],
      "source": [
        "reg2_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2RfJl_yMVHV"
      },
      "source": [
        "Обучаем нейронную сеть на полиномиальных зависимостях:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa57ZabbMVHV"
      },
      "outputs": [],
      "source": [
        "history = reg2_model.fit(\n",
        "    X2, y, \n",
        "    epochs=100,\n",
        "    # уровень выводимой информации\n",
        "    verbose=1,\n",
        "    # проверка (валидация) на 30% обучающих данных\n",
        "    validation_split = 0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmeZhfoyMVHW"
      },
      "source": [
        "Метод `fit` возвращает объект `history`, в котором для задачи регрессии обычно есть ключи `'loss'` и `'val_loss'`. Можно визуализировать историю обучения при помощи следующей функции:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uFZpfrDMVHW"
      },
      "outputs": [],
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.ylim([0, max(history.history['loss'])*0.5])\n",
        "  plt.title('Функция потерь при обучении модели')\n",
        "  plt.xlabel('Эпохи обучения')\n",
        "  plt.ylabel('Функция потерь')\n",
        "  plt.legend(['Обучающая выборка', 'Тестовая выборка'], loc='upper right')\n",
        "  plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3lDHZRDMVHW"
      },
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imVepALMMVHW"
      },
      "source": [
        "При помощи обученной модели можно выполнить прогноз:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztljTludMVHW"
      },
      "outputs": [],
      "source": [
        "y_predict_reg2 = reg2_model.predict(X2)\n",
        "\n",
        "plt.scatter(x, y, label='набор данных')\n",
        "plt.plot(np.sort(x), y_predict_reg2[np.argsort(x)], color='r', label='прогноз')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "359kP-S1MVHX"
      },
      "source": [
        "### Нелинейная парная регрессия при помощи TensorFlow\n",
        "\n",
        "Универсальная теорема аппроксимации утверждает, что любую непрерывную функцию можно с любой степенью точности аппроксимировать нейронной сетью с одним скрытым слоем с функцией активации сигмоида $\\sigma\\left(\\mathrm{x}\\right)=\\frac{1}{1+\\exp\\left(-\\mathrm{x}\\right)}$.\n",
        "\n",
        "Построим и обучим нейронную сеть такого типа для рассматриваемого набора данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WntnKc08MVHX"
      },
      "outputs": [],
      "source": [
        "uni_model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(1,)),\n",
        "    tf.keras.layers.Dense(units=512, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK45fUz-MVHX"
      },
      "outputs": [],
      "source": [
        "uni_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbGQNVq7MVHX"
      },
      "outputs": [],
      "source": [
        "uni_model.compile(loss='mse', \n",
        "                  optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "                  metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKqLpnUmMVHX"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjUm8eddMVHY"
      },
      "outputs": [],
      "source": [
        "history = uni_model.fit(\n",
        "    X, y, \n",
        "    epochs=100,\n",
        "    # уровень выводимой информации\n",
        "    verbose=1,\n",
        "    # проверка (валидация) на 20% обучающих данных\n",
        "    validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gyLHOLEMVHY"
      },
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2jkgVGRMVHY"
      },
      "outputs": [],
      "source": [
        "y_predict_uni = uni_model.predict(X)\n",
        "\n",
        "plt.scatter(x, y, label='набор данных')\n",
        "plt.plot(np.sort(x), y_predict_uni[np.argsort(x)], color='r', label='прогноз')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ0t-k1TMVHZ"
      },
      "source": [
        "### Нелинейная множественная регрессия при помощи TensorFlow\n",
        "\n",
        "При помощи нейронных сетей с нелинейными функциями активации нейронов можно успешно решать задачи нелинейной регрессии.\n",
        "\n",
        "Загрузим из TesorFlow Datasets набор `howell` с демографическими данными жителей Калахари:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA1kb7iCMVHZ"
      },
      "outputs": [],
      "source": [
        "ds = tfds.load(\"howell\", split='train')\n",
        "df = tfds.as_dataframe(ds)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rGyRyFvMVHZ"
      },
      "source": [
        "Изучим зависимость возраста от роста и веса:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be3yha_yMVHZ"
      },
      "outputs": [],
      "source": [
        "X = np.array(df[['height','weight']])\n",
        "y = np.array(df[['age']]).reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8C920a7MVHa"
      },
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b264pb3uMVHa"
      },
      "source": [
        "### Визуализация трехмерных данных\n",
        "\n",
        "Для построения 3d графиков необходимо импортировать необходимые модули:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqFNlxfPMVHa"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "# или from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLMbIU7zMVHa"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes(projection='3d') # или ax = fig.add_subplot(111, projection='3d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V76rqSwhMVHb"
      },
      "source": [
        "Для построения точечного графика можно использовать функцию `scatter()`, которой передаются три параметра для координат точек по осям X, Y и Z."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ46FLlnMVHb"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12,10)) \n",
        "ax = plt.axes(projection='3d') \n",
        "\n",
        "xs = X[:,0]\n",
        "ys = X[:,1]\n",
        "zs = y\n",
        "\n",
        "ax.scatter( xs, ys, zs, s=100 ) \n",
        "ax.set_xlabel('Рост') \n",
        "ax.set_ylabel('Вес') \n",
        "ax.set_zlabel('Возраст') \n",
        "ax.view_init( azim=-120, elev=25 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Ob0ipYMVHb"
      },
      "source": [
        "### Глубокая нейронная сеть для задачи регрессии\n",
        "\n",
        "Используем слой нормализации, адаптированный к обоим независимым признакам:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlApYLTvMVHb"
      },
      "outputs": [],
      "source": [
        "feature_normalizer = tf.keras.layers.Normalization(axis=None,input_shape=(2,)) \n",
        "feature_normalizer.adapt(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXocp0JUMVHc"
      },
      "source": [
        "Создадим нейронную сеть со слоем нормализации, четырьмя скрытыми плотными слоями с 64 нейронами и функцией активации ReLu и выходным слоем из одного нейрона:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2dM_UBrMVHc"
      },
      "outputs": [],
      "source": [
        "large_model = tf.keras.Sequential([\n",
        "    feature_normalizer,\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "large_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SXNqHwgMVHc"
      },
      "source": [
        "Скомпилируем модель, используя в качестве функции потерь  среднеквадратичную ошибку MSE с оптимизатором по умолчанию (RmsProp):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D-OOyieMVHc"
      },
      "outputs": [],
      "source": [
        "large_model.compile(loss='mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeQBNl0eMVHc"
      },
      "source": [
        "Обучим модель на наборе данных из двух признаков:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC1FAHs4MVHd"
      },
      "outputs": [],
      "source": [
        "history = large_model.fit(\n",
        "    X, y, \n",
        "    epochs=100,\n",
        "    # уровень выводимой информации\n",
        "    verbose=1,\n",
        "    # проверка (валидация) на 30% обучающих данных\n",
        "    validation_split = 0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvVKcYS8MVHd"
      },
      "source": [
        "Кривые обучения в зависимости от эпохи обучения выглядят так:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vtqio03MVHd"
      },
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNR-d2FFMVHd"
      },
      "source": [
        "Для визуализации прогнозируемых множественной регрессией значений воспользуемся функцией `plot_surface`. Потребуются определенные усилия по подготовке данных для `plot_surface`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO56vWymMVHe"
      },
      "outputs": [],
      "source": [
        "n_plot = 51\n",
        "\n",
        "x_plot = np.linspace(np.min(xs), np.max(xs), n_plot) \n",
        "y_plot = np.linspace(np.min(ys), np.max(ys), n_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HR2OVo1MVHe"
      },
      "outputs": [],
      "source": [
        "x_mesh, y_mesh = np.meshgrid(x_plot, y_plot)\n",
        "x_mesh.shape, y_mesh.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYRH9oyRMVHe"
      },
      "outputs": [],
      "source": [
        "x_plot2 = np.reshape(x_mesh, [n_plot**2,1])\n",
        "y_plot2 = np.reshape(y_mesh, [n_plot**2,1])\n",
        "xy_2 = np.hstack([x_plot2, y_plot2])\n",
        "xy_2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP3YgPPtMVHe"
      },
      "source": [
        "Теперь выполним прогнозирование при помощи обученной ранее модели, после чего вернемся к форме данных 51 на 51:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfjemZMjMVHe"
      },
      "outputs": [],
      "source": [
        "z = large_model.predict(xy_2)\n",
        "z.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDxaqBz5MVHf"
      },
      "outputs": [],
      "source": [
        "z_mesh = z.reshape((n_plot, n_plot))\n",
        "z_mesh.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH-2RAhSMVHf"
      },
      "source": [
        "Функция `plot_surface` имеет большое число параметров, в частности:\n",
        "\n",
        "* X, Y, Z : 2D массивы – данные для построения поверхности.\n",
        "* rstride, cstride : int – параметры определяют величину шага, с которым будут браться элементы строки/столбца из переданных массивов.\n",
        "* cmap: Colormap – цветовая карта для элементов поверхности.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbEbSiMdMVHf"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "surf = ax.plot_surface(x_mesh, y_mesh, z_mesh, \\\n",
        "       rstride=1, cstride=1, linewidth=0.05, cmap=cm.winter, antialiased=True, \\\n",
        "       edgecolors='gray') \n",
        "ax.scatter( xs, ys, zs, s=100, c='r' )\n",
        "\n",
        "ax.set_xlabel('Рост', fontsize=14) \n",
        "ax.set_ylabel('Вес', fontsize=14)\n",
        "ax.set_zlabel('Возраст', fontsize=14) \n",
        "ax.set_title('Демографические данные обитателей Калахари', fontsize=16)\n",
        "\n",
        "ax.set_zlim(0., z_mesh.max())\n",
        "ax.view_init(elev = 20, azim = 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkAXqXKrMVHf"
      },
      "source": [
        "__Кривые обучения__ — это графическое представление зависимости меры (показателя) качества обучения (по вертикальной оси) от определенного показателя модели обучения (по горизонтальной оси). Будем визуализировать в качестве качества модели показатели RMSE для части обучающей выборки и тестовой выборки в зависимости от количества точек в обучающей выборке.\n",
        "\n",
        "Для разбиения на обучающую и тестовую выборки можно использовать функцию `train_test_split`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g4AjpjqMVHg"
      },
      "outputs": [],
      "source": [
        "def train_test_split(X, y, test_ratio=0.2, seed=None):\n",
        "    \"\"\"возвращает X_train, X_test, y_train, y_test\"\"\"\n",
        "    assert X.shape[0] == y.shape[0], \\\n",
        "        \"Размер X должен быть равен размеру y\"\n",
        "    assert 0.0 <= test_ratio <= 1.0, \\\n",
        "        \"Неверное значение test_ratio\"\n",
        "\n",
        "    if seed:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    shuffled_indexes = np.random.permutation(len(X))\n",
        "\n",
        "    test_size = int(len(X) * test_ratio)\n",
        "    test_indexes = shuffled_indexes[:test_size]\n",
        "    train_indexes = shuffled_indexes[test_size:]\n",
        "\n",
        "    X_train = X[train_indexes]\n",
        "    y_train = y[train_indexes]\n",
        "\n",
        "    X_test = X[test_indexes]\n",
        "    y_test = y[test_indexes]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_-nPW_WMVHg"
      },
      "source": [
        "Разобьем массивы данных `X` и `y` на обучающие и тестовые данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHZzvgwYMVHg"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, 0.3)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будем использовать для визуализации показатель RMSE:"
      ],
      "metadata": {
        "id": "nihQJd3k2Lr6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au5gMSCJMVHg"
      },
      "outputs": [],
      "source": [
        "def my_rmse(y_test, y_predict):\n",
        "    return np.sqrt(np.sum((y_predict - y_test)**2) / len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Всего в обучающей выборке 381 точка, начнем с 11 точек и будем прибавлять по 10 точек на каждом шаге цикла:"
      ],
      "metadata": {
        "id": "DGI1hWnG2afA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OpqX55pMVHg"
      },
      "outputs": [],
      "source": [
        "train_score = []\n",
        "test_score = []\n",
        "for i in range(11, 381, 10):\n",
        "    large_model = tf.keras.Sequential([\n",
        "        feature_normalizer,\n",
        "#        tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "#        tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "#        tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=1)\n",
        "    ])\n",
        "    large_model.compile(loss='mse')\n",
        "    large_model.fit(X_train[:i], y_train[:i], epochs=50, verbose=0)\n",
        "\n",
        "    y_train_predict = large_model.predict(X_train[:i])\n",
        "    train_score.append(my_rmse(y_train[:i], y_train_predict))\n",
        "    \n",
        "    y_test_predict = large_model.predict(X_test)\n",
        "    test_score.append(my_rmse(y_test, y_test_predict))\n",
        "    print('-->', i, ' done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUgu3e7tMVHh"
      },
      "outputs": [],
      "source": [
        "plt.plot([i for i in range(11, len(X_train), 10)], \n",
        "                               train_score, label=\"train\")\n",
        "plt.plot([i for i in range(11, len(X_train), 10)], \n",
        "                               test_score, label=\"test\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvrOTmXEBUVS"
      },
      "source": [
        "#### Задание (10 баллов)\n",
        "\n",
        "Для закрепленного за Вами варианта лабораторной работы:\n",
        "\n",
        "1.\tЗагрузите заданный в индивидуальном задании набор данных из Tensorflow Datasets, включая указанные в задании независимый признак и зависимый признак (отклик).\n",
        "\n",
        "2.\tРешите задачу полиномиальной регрессии для степени полинома, указанной в индивидуальном задании, при помощи нейронной сети с одним нейроном и оцените качество полученной модели по показателю, указанному в индивидуальном задании. \n",
        "\n",
        "3.\tПостройте кривые обучения с зависимостью от количества эпох.\n",
        "\n",
        "4.\tВизуализируйте точки набора данных на плоскости в виде диаграммы рассеяния (ось X – независимый признак, ось Y – зависимый признак), а также линию регрессии (другим цветом), подписывая оси и рисунок. \n",
        "\n",
        "5.\tОпределите в исходном наборе данных признак (отличный от независимого и зависимого признаков), принимающий непрерывные значения и имеющий свойства, указанные в индивидуальном задании. \n",
        "\n",
        "6.\tВизуализируйте этот признак в соответствии с индивидуальным заданием. \n",
        "\n",
        "7.\tСформируйте набор входных данных из двух признаков набора данных (независимый признак и определенный признак), создайте и адаптируйте нормализующий слой Tensorflow для двух признаков. \n",
        "\n",
        "8.\tИспользуя созданный нормализующий слой, постройте нейронную сеть (нелинейный регресор) с количеством скрытых слоев, количеством нейронов и функцией активации, указанными в индивидуальном задании, и одним нейроном в выходном слое и обучите ее на наборе данных из двух признаков и отклика. \n",
        "\n",
        "9.\tВизуализируйте набор данных в виде точечного графика и прогноз нейронной сети в виде поверхности в трехмерном пространстве.\n",
        "\n",
        "10.\tРазбейте набор данных из двух признаков и отклика на обучающую и тестовую выборки и постройте кривые обучения для заданного показателя качества в зависимости от количества точек в обучающей выборке, подписывая оси и рисунок и создавая легенду.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgSiniL9MVHh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}